// # Faster Paxos
//
// Faster Paxos is a multi-leader MultiPaxos variant that incorporates aspects
// of Fast Paxos and Mencius. The protocol consists of an arbitrary number of
// _clients_ and 2f+1 _servers_. In a given round, one server acts as a
// _leader_ and picks picks f+1 _delegate leaders_. Depending on its
// role---leader, delegate, or plain old server---a server acts as some
// combination of a proposer, acceptor, and replica. Here is a protocol
// cheatsheet, similar to [1].
//
// ## Changing Rounds
//
//     Client      Leader     Delegate     Server
//       |           | ----------^---------> |      Phase1a
//       |           | <---------^---------- |      Phase1b
//       |           | ----------^---------> |      Phase2a
//       |           | <---------^---------- |      Phase2b
//       |           | ----------^---------> |      Phase3a
//       |           | --------> |           |      Phase2aAny
//       |           | <-------- |           |      Phase2aAnyAck
//
// ## Normal Operation
//
//     Client      Leader     Delegate     Server
//       | ----------^---------> |           |      ClientRequest
//       |           |           | ------.   |      Phase2a
//       |           |           | <-----'   |
//       |           |           | ------.   |      Phase2b
//       |           |           | <-----'   |
//       |           |           | --------> |      Phase3a
//       | <---------^---------- |           |      ClientReply
//
// ## Learning Who the Delegates Are
//
// TODO(mwhittaker): Update text.
// Clients can send their commands to any of the delegates, but how do the
// clients know who the delegates are? Every client keeps track of what it
// thinks is the current round and current set of delegates. When a client
// sends a command to a node (that it thinks is a delegate), it includes the
// round. If the round is equal to the round of the node, great. If the round
// is smaller, then the client is stale. The node tells the client that it is
// in a stale round. The client then broadcasts to all nodes to figure out what
// the current round and set of delegates are. Only the leader responds to this
// broadcast.
//
//     Client      Leader     Delegate     Server
//       | ----------^---------> |           |      ClientRequest
//       | <---------^---------- |           |      RoundInfo
//
// ## Knowing When to Change Rounds
//
// In MultiPaxos, we elect a stable leader to orchestrate the protocol. As long
// as the leader doesn't fail, we never have to change rounds. If the leader
// does fail, however, a different node has to detect the failure and change to
// a larger round to elect a new leader.
//
// With Faster Paxos, this is a little more complicated. If _any_ of the
// delegates fail, then we have to switch rounds. Thus, nodes have to monitor
// all of the delegates. But how does a node even know who the delegates are?
//
// To solve this, we first establish the invariant that if a node is in round r
// in any capacity (i.e. as a leader, a delegate, or server), it knows the
// delegates. To establish this invariant, when a leader begins a round, it
// immediately picks a set of delegates. Then, when it contacts the other
// servers for the first time, it includes the delegates.
//
// Second, we use all-to-all heartbeats so that every server monitors every
// other server. If a server ever detects that a node in the current set of
// delegates is dead, then it changes to a larger round and attempts to become
// leader. We use the same heartbeats to ensure that a leader doesn't choose a
// delegate that it thinks has failed.
//
// TODO(mwhittaker): Is this live?
//
// ## Recovering Pending Commands
//
// A server's log may have a hole in it. If the hole isn't filled in for quite
// some time, the server contacts the delegate that owns the log entry and asks
// if a value has been chosen there yet (we call this a Recover message). If a
// value has been chosen, the delegate lets the server know. Otherwise, it
// ignores the message and will eventually let the server know whenever the
// hole gets filled.
//
//     Client      Leader     Delegate     Server
//       |           |           | <-------- |      Recover
//       |           |           | --------> |      Phase3a
//
// ## Nacks
//
// Faster Paxos has two kinds of nacks. The first kind is the standard Paxos
// nack that an acceptor sends when it receives a message from a server with a
// stale round. The second kind is specific to Faster Paxos. Say a delegate A
// sends a Phase2a message with a noop in slot i to another delegate B. When B
// receives the message, if it has already voted for some non-noop value in
// slot i, then it sends back the Phase2b for the non-noop value to A. If A
// receives Phase2b's from every other delegate, then it can also vote for the
// non-noop value and immediately know the value is chosen.
//
//     Client      Leader     Delegate     Server
//       |           | ----------^---------> |      Phase1a
//       |           | <---------^---------- |      Nack
//       |           | ----------^---------> |      Phase2a
//       |           | <---------^---------- |      Nack
//       |           |           | ------.   |      Phase2b
//       |           |           | <-----'   |
//       |           |           | ------.   |      Nack
//       |           |           | <-----'   |
//       |           |           | ------.   |      Phase2a (noop)
//       |           |           | <-----'   |
//       |           |           | ------.   |      Phase2b (non-noop)
//       |           |           | <-----'   |
//
// ## Simplifying Assumptions
//
// - We assume that a leader always chooses itself to be one of the delegates.
//
// [1]: https://ndpsoftware.com/git-cheatsheet.html

syntax = "proto2";

package frankenpaxos.fasterpaxos;

import "scalapb/scalapb.proto";

option (scalapb.options) = {
  package_name: "frankenpaxos.fasterpaxos"
  flat_package: true
};

// Helper messages. ////////////////////////////////////////////////////////////
message Noop {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";
}

message CommandId {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  // A client's address, pseudonym, and id uniquely identify a command. See
  // Client.java for more information.
  required bytes client_address = 1;
  required int32 client_pseudonym = 2;
  required int32 client_id = 3;
}

message Command {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required CommandId command_id = 1;
  required bytes command = 2;
}

message CommandOrNoop {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof value {
    Command command = 1;
    Noop noop = 2;
  }
}

// Protocol messages. //////////////////////////////////////////////////////////
message ClientRequest {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 round = 1;
  required Command command = 2;
}

message ClientReply {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required CommandId command_id = 1;
  required bytes result = 2;
}

message Phase1a {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 round = 1;

  // The leader knows that all entries in slots less than `chosenWatermark`
  // have been chosen. Acceptors do not have to include slots below
  // `chosenWatermark` in their phase1b response.
  //
  // The leader may know that some entries larger than `chosenWatermark` have
  // also been chosen, but that's okay. It's not unsafe for acceptors to return
  // too much information.
  required int32 chosen_watermark = 2;

  // The list of delegates (technically the indexes of the delegates) for this
  // round.
  repeated int32 delegate = 3;
}

message PendingSlotInfo {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 vote_round = 1;
  required CommandOrNoop vote_value = 2;
}

message ChosenSlotInfo {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required CommandOrNoop value = 1;
}

message Phase1bSlotInfo {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 slot = 1;
  oneof info {
    PendingSlotInfo pending_slot_info = 2;
    ChosenSlotInfo chosen_slot_info = 3;
  }
}

message Phase1b {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 server_index = 1;
  required int32 round = 2;
  repeated Phase1bSlotInfo info = 3;
}

message Phase2a {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 slot = 1;
  required int32 round = 2;
  required CommandOrNoop command_or_noop = 3;
}

message Phase2b {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 server_index = 1;
  required int32 slot = 2;
  required int32 round = 3;

  // Here is an optimization to Faster Paxos. Assume a delegate A sends a
  // Phase2a with a noop to another delegate B in slot s. Assume that when B
  // receives the noop, it has already voted for a command in slot s. That is,
  // it has already sent back a Phase2b for some command. B can send the same
  // Phase2b to A. This way, A learns of the command sooner. This is only an
  // optimization. B can instead ignore the Phase2a from A and the protocol
  // will work just fine. If we do this optimization, then B includes the
  // command here. Otherwise, it's empty.
  optional Command command = 4;
}

message Phase2aAny {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 round = 1;
  repeated int32 delegate = 2;

  // A delegate can propose a value in any log entry any_watermark or larger.
  // These are the log entries for which the leader determined that no value
  // has been or will be chosen in any round less than `round`.
  required int32 any_watermark = 3;
}

message Phase2aAnyAck {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 round = 1;
  required int32 server_index = 2;
}

message Phase3a {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 slot = 1;
  required CommandOrNoop command_or_noop = 2;
}

// message StaleRound {
//   option (scalapb.message).annotations =
//     "@scala.scalajs.js.annotation.JSExportAll";
// }

// message RoundInfoRequest {
//   option (scalapb.message).annotations =
//     "@scala.scalajs.js.annotation.JSExportAll";
// }

message RoundInfo {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 round = 1;
  repeated int32 delegate = 2;
}

message Nack {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  // When a server receives a message with a round smaller than its own, it
  // sends back its round in a Nack.
  required int32 round = 1;
}

message Recover {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 slot = 1;
}

// Inbound messages. ///////////////////////////////////////////////////////////
message ClientInbound {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof request {
    ClientReply client_reply = 1;
    RoundInfo round_info = 2;
  }
}

message ServerInbound {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof request {
    ClientRequest client_request = 1;
    Phase1a phase1a = 2;
    Phase1b phase1b = 3;
    Phase2a phase2a = 4;
    Phase2b phase2b = 5;
    Phase2aAny phase2a_any = 6;
    Phase2aAnyAck phase2a_any_ack = 7;
    Phase3a phase3a = 8;
    Recover recover = 10;
    Nack nack = 11;
  }
}
