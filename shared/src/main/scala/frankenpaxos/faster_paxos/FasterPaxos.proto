// # Faster Paxos
//
// Faster Paxos is a multi-leader MultiPaxos variant that incorporates aspects
// of Fast Paxos and Mencius. The protocol consists of an arbitrary number of
// _clients_ and 2f+1 _servers_. In a given round, one server acts as a
// _leader_ and picks picks f+1 _delegate leaders_. Depending on its
// role---leader, delegate, or plain old server---a server acts as some
// combination of a proposer, acceptor, and replica. Here is a protocol
// cheatsheet, similar to [1].
//
// ## Changing Rounds
//
//     Client      Leader     Delegate     Server
//       |           | ----------^---------> |      Phase1a
//       |           | <---------^---------- |      Phase1b
//       |           | ----------^---------> |      Phase2a
//       |           | <---------^---------- |      Phase2b
//       |           | ----------^---------> |      Phase3a
//       |           | --------> |           |      Phase2aAny
//
// ## Normal Operation
//
//     Client      Leader     Delegate     Server
//       | ----------^---------> |           |      ClientRequest
//       |           |           | ------.   |      Phase2a
//       |           |           | <-----'   |
//       |           |           | ------.   |      Phase2b
//       |           |           | <-----'   |
//       |           |           | --------> |      Phase3a
//       | <---------^---------- |           |      ClientReply
//
// ## Learning Who the Delegates Are
//
// Clients can send their commands to any of the delegates, but how do the
// clients know who the delegates are? Every client keeps track of what it
// thinks is the current round and current set of delegates. When a client
// sends a command to a node (that it thinks is a delegate), it includes the
// round. If the round is equal to the round of the node, great. If the round
// is smaller, then the client is stale. The node tells the client that it is
// in a stale round. The client then broadcasts to all nodes to figure out what
// the current round and set of delegates are. Only the leader responds to this
// broadcast.
//
//     Client      Leader     Delegate     Server
//       | ----------^---------> |           |      ClientRequest
//       | <---------^---------- |           |      StaleRound
//       | ----------^-----------^---------> |      RoundInfoRequest
//       | <-------- |           |           |      RoundInfoReply
//
// ## Knowing When to Change Rounds
//
// In MultiPaxos, we elect a stable leader to orchestrate the protocol. As long
// as the leader doesn't fail, we never have to change rounds. If the leader
// does fail, however, a different node has to detect the failure and change to
// a larger round to elect a new leader.
//
// With Faster Paxos, this is a little more complicated. If _any_ of the
// delegates fail, then we have to switch rounds. Thus, nodes have to monitor
// all of the delegates. But how does a node even know who the delegates are?
//
// To solve this, we first establish the invariant that if a node is in round r
// in any capacity (i.e. as a leader, a delegate, or server), it knows the
// delegates. To establish this invariant, when a leader begins a round, it
// immediately picks a set of delegates. Then, when it contacts the other
// servers for the first time, it includes the delegates.
//
// Second, we use all-to-all heartbeats so that every server monitors every
// other server. If a server ever detects that a node in the current set of
// delegates is dead, then it changes to a larger round and attempts to become
// leader. We use the same heartbeats to ensure that a leader doesn't choose a
// delegate that it thinks has failed.
//
// TODO(mwhittaker): Is this live?
//
// ## Recovering Pending Commands
//
// A server's log may have a hole in it. If the hole isn't filled in for quite
// some time, the server contacts the delegate that owns the log entry and asks
// if a value has been chosen there yet (we call this a Recover message). If a
// value has been chosen, the delegate lets the server know. Otherwise, it
// ignores the message and will eventually let the server know whenever the
// hole gets filled.
//
//     Client      Leader     Delegate     Server
//       |           |           | <-------- |      Recover
//       |           |           | --------> |      Phase3a
//
// ## Nacks
//
// Faster Paxos has two kinds of nacks. The first kind is the standard Paxos
// nack that an acceptor sends when it receives a message from a server with a
// stale round. The second kind is specific to Faster Paxos. Say a delegate A
// sends a Phase2a message with a noop in slot i to another delegate B. When B
// receives the message, if it has already voted for some non-noop value in
// slot i, then it sends back the Phase2b for the non-noop value to A. If A
// receives Phase2b's from every other delegate, then it can also vote for the
// non-noop value and immediately know the value is chosen.
//
//     Client      Leader     Delegate     Server
//       |           | ----------^---------> |      Phase1a
//       |           | <---------^---------- |      Nack
//       |           | ----------^---------> |      Phase2a
//       |           | <---------^---------- |      Nack
//       |           |           | ------.   |      Phase2b
//       |           |           | <-----'   |
//       |           |           | ------.   |      Nack
//       |           |           | <-----'   |
//       |           |           | ------.   |      Phase2a (noop)
//       |           |           | <-----'   |
//       |           |           | ------.   |      Phase2b (non-noop)
//       |           |           | <-----'   |
//
// ## Simplifying Assumptions
//
// - We assume that a leader always chooses itself to be one of the delegates.
//
// [1]: https://ndpsoftware.com/git-cheatsheet.html

syntax = "proto2";

package frankenpaxos.faster_paxos;

import "scalapb/scalapb.proto";

option (scalapb.options) = {
  package_name: "frankenpaxos.faster_paxos"
  flat_package: true
};

// Helper messages. ////////////////////////////////////////////////////////////
message Noop {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";
}

message CommandId {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  // A client's address, pseudonym, and id uniquely identify a command.
  required bytes client_address = 1;
  required int32 client_pseudonym = 2;
  required int32 client_id = 3;
}

message Command {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required CommandId command_id = 1;
  required bytes command = 2;
}

message CommandBatch {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  repeated Command command = 1;
}

message CommandBatchOrNoop {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof value {
    CommandBatch command_batch = 1;
    Noop noop = 2;
  }
}

// Protocol messages. //////////////////////////////////////////////////////////
message ClientRequest {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required Command command = 1;
}

message ClientRequestBatch {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required CommandBatch batch = 1;
}

message Phase1a {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 round = 1;

  // The leader knows that all entries in slots less than `chosenWatermark`
  // have been chosen. Acceptors do not have to include slots below
  // `chosenWatermark` in their phase1b response.
  //
  // The leader may know that some entries larger than `chosenWatermark` have
  // also been chosen, but that's okay. It's not unsafe for acceptors to return
  // too much information.
  required int32 chosen_watermark = 2;
}

message Phase1bSlotInfo {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 slot = 1;
  required int32 vote_round = 2;
  required CommandBatchOrNoop vote_value = 3;
}

message Phase1b {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 group_index = 1;
  required int32 acceptor_index = 2;
  required int32 round = 3;
  repeated Phase1bSlotInfo info = 4;
}

message Phase2a {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 slot = 1;
  required int32 round = 2;
  required CommandBatchOrNoop command_batch_or_noop = 3;
}

message Phase2b {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 acceptor_index = 1;
  required int32 slot = 2;
  required int32 round = 3;
}

message Chosen {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 slot = 1;
  required CommandBatchOrNoop command_batch_or_noop = 2;
}

message ClientReply {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required CommandId command_id = 1;
  required bytes result = 2;
}

message ClientReplyBatch {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  repeated ClientReply batch = 1;
}

message MaxSlotRequest {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required CommandId command_id = 1;
}

message MaxSlotReply {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required CommandId command_id = 1;
  required int32 acceptor_index = 2;
  required int32 slot = 3;
}

message ReadRequest {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 slot = 1;
  required Command command = 2;
}

message ReadReply {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required CommandId command_id = 1;
  required bytes result = 2;
}

// If a client or batcher sends a request to a leader, but the leader is
// inactive, then the leader sends back a NotLeader{Client,Batcher} message.
// The client or batcher then sends a LeaderInfoRequest{Client,Batcher} request
// to all leaders, and the active leader replies with a
// LeaderInfoReply{Client,Batcher} request with its current round.
message NotLeaderClient {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";
}

message LeaderInfoRequestClient {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";
}

message LeaderInfoReplyClient {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 round = 1;
}

message NotLeaderBatcher {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required ClientRequestBatch client_request_batch = 1;
}

message LeaderInfoRequestBatcher {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";
}

message LeaderInfoReplyBatcher {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 round = 1;
}

message Nack {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 round = 1;
}

message ChosenWatermark {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  // Replicas periodically send ChosenWatermark messages to the leaders
  // informing them that every log entry smaller than `slot` has been chosen.
  // For example, if `slot` is 3, then slots 0, 1, and 2 have been chosen.
  // Slots above `slot` may also be chosen, but that's okay.
  //
  // If replicas didn't send these messages, then leaders would have no idea
  // which commands have been chosen and which haven't. This can significantly
  // slow things down after a leader change.
  required int32 slot = 1;
}

message Recover {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  // Replicas execute logs in prefix order. Thus, if the log permanently has a
  // hole in it, the algorithm remains forever blocked. To solve this, if a
  // replica notices a hole in its log for a certain amount of time, it sends a
  // Recover message to the leader to get the hole plugged.
  required int32 slot = 1;
}

// Inbound messages. ///////////////////////////////////////////////////////////
message ClientInbound {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof request {
    ClientReply client_reply = 1;
    NotLeaderClient not_leader_client = 2;
    LeaderInfoReplyClient leader_info_reply_client = 3;
    MaxSlotReply max_slot_reply = 4;
    ReadReply read_reply = 5;
  }
}

message BatcherInbound {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof request {
    ClientRequest client_request = 1;
    NotLeaderBatcher not_leader_batcher = 2;
    LeaderInfoReplyBatcher leader_info_reply_batcher = 3;
  }
}

message LeaderInbound {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof request {
    Phase1b phase1b = 1;
    ClientRequest client_request = 2;
    ClientRequestBatch client_request_batch = 3;
    LeaderInfoRequestClient leader_info_request_client = 4;
    LeaderInfoRequestBatcher leader_info_request_batcher = 5;
    Nack nack = 6;
    ChosenWatermark chosen_watermark = 7;
    Recover recover = 8;
  }
}

message ProxyLeaderInbound {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof request {
    Phase2a phase2a = 1;
    Phase2b phase2b = 2;
  }
}

message AcceptorInbound {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof request {
    Phase1a phase1a = 1;
    Phase2a phase2a = 2;
    MaxSlotRequest max_slot_request = 3;
  }
}

message ReplicaInbound {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof request {
    Chosen chosen = 1;
    ReadRequest read_request = 2;
  }
}

message ProxyReplicaInbound {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof request {
    ClientReplyBatch client_reply_batch = 1;
    ChosenWatermark chosen_watermark = 2;
    Recover recover = 3;
  }
}
